% This file was created with Citavi 6.0.0.2

@book{.,
 title = {1th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy16)}
}


@phdthesis{Krause.,
 author = {Krause, Josua Walter Hugo},
 title = {Using Visual Analytics to Explain Black-Box Machine Learning},
 school = {{New York University Tandon School of Engineering}}
}


@inproceedings{Krishna.2016,
 author = {Krishna, Rahul and Menzies, Tim and Fu, Wei},
 title = {Too Much Automation? The Bellwether Effect and Its Implications for Transfer Learning},
 url = {http://doi.acm.org/10.1145/2970276.2970339},
 keywords = {Data Mining;Defect Prediction;Transfer learning},
 pages = {122--131},
 publisher = {ACM},
 isbn = {978-1-4503-3845-5},
 series = {ASE 2016},
 booktitle = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/2970276.2970339}
}


@proceedings{Kuhnberger.2013,
 year = {2013},
 title = {Artificial General Intelligence},
 address = {Berlin, Heidelberg},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-39521-5},
 editor = {K{\"u}hnberger, Kai-Uwe and Rudolph, Sebastian and Wang, Pei}
}


@inproceedings{Kumar.2017,
 author = {Kumar, Devinder and Wong, Alexander and Taylor, Graham W.},
 title = {Explaining the unexplained: A class-enhanced attentive response (clear) approach to understanding deep neural networks},
 booktitle = {IEEE Computer Vision and Pattern Recognition (CVPR) Workshop},
 year = {2017}
}


@article{KurtHornik.1989,
 author = {{Kurt Hornik} and {Maxwell Stinchcombe} and {Halbert White}},
 year = {1989},
 title = {Multilayer feedforward networks are universal approximators},
 url = {http://www.sciencedirect.com/science/article/pii/0893608089900208},
 keywords = {Back-propagation networks;Feedforward networks;Mapping networks;Network representation capability;Sigma-Pi networks;Squashing functions;Stone-Weierstrass Theorem;Universal approximation},
 pages = {359--366},
 volume = {2},
 number = {5},
 issn = {0893-6080},
 journal = {Neural Networks},
 doi = {10.1016/0893-6080(89)90020-8}
}


@article{Kurtz.2003,
 author = {Kurtz, Cynthia F. and Snowden, David J.},
 year = {2003},
 title = {The new dynamics of strategy: Sense-making in a complex and complicated world},
 pages = {462--483},
 volume = {42},
 number = {3},
 journal = {IBM systems journal}
}


@inproceedings{Labaf.2017,
 author = {Labaf, Maryam and Hitzler, Pascal and Evans, Anthony B.},
 title = {Propositional rule extraction from neural networks under background knowledge},
 keywords = {Rule extraction},
 volume = {17},
 booktitle = {Proceedings of the Twelfth International Workshop on Neural-Symbolic Learning and Reasoning, NeSy},
 year = {2017}
}


@incollection{Lazaric.2012,
 abstract = {Transfer in reinforcement learning is a novel research area that focuses on the development of methods to transfer knowledge from a set of source tasks to a target task. Whenever the tasks are similar, the transferred knowledge can be used by a learning algorithm to solve the target task and significantly improve its performance (e.g., by reducing the number of samples needed to achieve a nearly optimal performance). In this chapter we provide a formalization of the general transfer problem, we identify the main settings which have been investigated so far, and we review the most important approaches to transfer in reinforcement learning.},
 author = {Lazaric, Alessandro},
 title = {Transfer in Reinforcement Learning: A Framework and a Survey},
 keywords = {Reinforcement Learning},
 pages = {143--173},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-27645-3},
 editor = {Wiering, Marco and {van Otterlo}, Martijn},
 booktitle = {Reinforcement Learning: State-of-the-Art},
 year = {2012},
 address = {Berlin, Heidelberg},
 doi = {10.1007/978-3-642-27645-3{\textunderscore }5}
}


@article{Lehmann.2010,
 abstract = {Artificial neural networks can be trained to perform excellently in many application areas. Whilst they can learn from raw data to solve sophisticated recognition and analysis problems, the acquired knowledge remains hidden within the network architecture and is not readily accessible for analysis or further use: Trained networks are black boxes. Recent research efforts therefore investigate the possibility to extract symbolic knowledge from trained networks, in order to analyze, validate, and reuse the structural insights gained implicitly during the training process. In this paper, we will study how knowledge in form of propositional logic programs can be obtained in such a way that the programs are as simple as possible---where simple is being understood in some clearly defined and meaningful way.},
 author = {Lehmann, Jens and Bader, Sebastian and Hitzler, Pascal},
 year = {2010},
 title = {Extracting reduced logic programs from artificial neural networks},
 keywords = {Rule extraction},
 pages = {249--266},
 volume = {32},
 number = {3},
 issn = {1573-7497},
 journal = {Applied Intelligence},
 doi = {10.1007/s10489-008-0142-y}
}


@article{Lei.2016,
 author = {Lei, Tao and Barzilay, Regina and Jaakkola, Tommi},
 year = {2016},
 title = {Rationalizing neural predictions},
 journal = {arXiv preprint arXiv:1606.04155}
}


@article{Letham.2015,
 author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David and others},
 year = {2015},
 title = {Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model},
 pages = {1350--1371},
 volume = {9},
 number = {3},
 journal = {The Annals of Applied Statistics}
}


@article{Letham.2015b,
 author = {Letham, Benjamin and Rudin, Cynthia and McCormick, Tyler H. and Madigan, David and others},
 year = {2015},
 title = {Interpretable classifiers using rules and bayesian analysis: Building a better stroke prediction model},
 pages = {1350--1371},
 volume = {9},
 number = {3},
 journal = {The Annals of Applied Statistics}
}


@article{Lipton.2016,
 author = {Lipton, Zachary C.},
 year = {2016},
 title = {The mythos of model interpretability},
 journal = {arXiv preprint arXiv:1606.03490}
}


@article{Lundberg.2016,
 author = {Lundberg, Scott and Lee, Su-In},
 year = {2016},
 title = {An unexpected unity among methods for interpreting model predictions},
 journal = {arXiv preprint arXiv:1611.07478}
}


@inproceedings{Luo.2008,
 author = {Luo, Ping and Zhuang, Fuzhen and Xiong, Hui and Xiong, Yuhong and He, Qing},
 title = {Transfer Learning from Multiple Source Domains via Consensus Regularization},
 url = {http://doi.acm.org/10.1145/1458082.1458099},
 keywords = {classification;consensus regularization;Transfer learning},
 pages = {103--112},
 publisher = {ACM},
 isbn = {978-1-59593-991-3},
 series = {CIKM '08},
 booktitle = {Proceedings of the 17th ACM Conference on Information and Knowledge Management},
 year = {2008},
 address = {New York, NY, USA},
 doi = {10.1145/1458082.1458099}
}


@inproceedings{Krause.2016,
 author = {Krause, Josua and Perer, Adam and Ng, Kenney},
 title = {Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models},
 url = {http://doi.acm.org/10.1145/2858036.2858529},
 keywords = {interactive machine learning;partial dependence;predictive modeling},
 pages = {5686--5697},
 publisher = {ACM},
 isbn = {978-1-4503-3362-7},
 series = {CHI '16},
 booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/2858036.2858529}
}


@article{Luo.2018,
 author = {Luo, Yunan and Ma, Jianzhu and Liu, Yang and Ye, Qing and Ideker, Trey and Peng, Jian},
 year = {2018},
 title = {Deciphering signaling specificity with interpretable deep neural networks},
 pages = {288647},
 journal = {bioRxiv}
}


@inproceedings{Krause.2016b,
 author = {Krause, Josua and Perer, Adam and Ng, Kenney},
 title = {Interacting with Predictions: Visual Inspection of Black-box Machine Learning Models},
 url = {http://doi.acm.org/10.1145/2858036.2858529},
 keywords = {interactive machine learning;partial dependence;predictive modeling},
 pages = {5686--5697},
 publisher = {ACM},
 isbn = {978-1-4503-3362-7},
 series = {CHI '16},
 booktitle = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
 year = {2016},
 address = {New York, NY, USA},
 doi = {10.1145/2858036.2858529}
}


@book{Kolman.2008,
 author = {Kolman, Eyal and Margaliot, Michael},
 year = {2008},
 title = {Knowledge-based neurocomputing: a fuzzy logic approach},
 volume = {234},
 publisher = {Springer}
}


@article{Hirsh.1994,
 author = {Hirsh, Haym and Noordewier, Michiel},
 year = {1994},
 title = {Using background knowledge to improve inductive learning: a case study in molecular biology},
 pages = {3--6},
 volume = {9},
 number = {5},
 journal = {IEEE Expert}
}


@incollection{HoaT.Le.,
 author = {{Hoa T. Le} and {Christophe Cerisara} and {Alexandre Denis}},
 title = {Do Convolutional Networks need to be Deep for Text Classification ?},
 url = {https://sites.google.com/view/affcon18/home},
 booktitle = {The AAAI-18 Workshop on Affective Content Analysis (AFFCON2018)}
}


@inproceedings{Hu.2016,
 author = {Hu, Zhiting and Ma, Xuezhe and Liu, Zhengzhong and Hovy, Eduard and Xing, Eric},
 title = {Harnessing Deep Neural Networks with Logic Rules},
 url = {http://www.aclweb.org/anthology/P16-1228},
 pages = {2410--2420},
 publisher = {{Association for Computational Linguistics}},
 booktitle = {Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)},
 year = {2016},
 doi = {10.18653/v1/P16-1228}
}


@proceedings{IEEE.1997,
 year = {1997},
 title = {Neural Networks, 1997., International Conference on},
 institution = {IEEE}
}


@proceedings{IEEE.2013,
 year = {2013},
 title = {Computational Intelligence for Human-like Intelligence (CIHLI), 2013 IEEE Symposium on},
 institution = {IEEE}
}


@proceedings{IEEE.2014,
 year = {2014},
 title = {2014 IEEE International Conference on Data Mining (ICDM)},
 institution = {IEEE}
}


@proceedings{IEEE.2016,
 year = {2016},
 title = {Security and Privacy (SP), 2016 IEEE Symposium on},
 institution = {IEEE}
}


@proceedings{IEEE.2017,
 year = {2017},
 title = {Signal Processing Conference (EUSIPCO), 2017 25th European},
 institution = {IEEE}
}


@article{JurgenSchmidhuber.2015,
 author = {{J{\"u}rgen Schmidhuber}},
 year = {2015},
 title = {Deep learning in neural networks: An overview},
 url = {http://www.sciencedirect.com/science/article/pii/S0893608014002135},
 keywords = {deep learning;Evolutionary computation;literature review;Reinforcement Learning;Supervised learning;Unsupervised learning},
 pages = {85--117},
 volume = {61},
 issn = {0893-6080},
 journal = {Neural Networks},
 doi = {10.1016/j.neunet.2014.09.003}
}


@inproceedings{Kahng.2016,
 author = {Kahng, Minsuk and Fang, Dezhi and Chau, Duen Horng Polo},
 title = {Visual exploration of machine learning results using data cube analysis},
 pages = {1},
 booktitle = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
 year = {2016}
}


@article{Karpathy.2015,
 author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
 year = {2015},
 title = {Visualizing and understanding recurrent networks},
 keywords = {recurrent},
 journal = {arXiv preprint arXiv:1506.02078}
}


@incollection{Kim.2015,
 author = {Kim, Been and Shah, Julie A. and Doshi-Velez, Finale},
 title = {Mind the Gap: A Generative Approach to Interpretable Feature Selection and Extraction},
 url = {http://papers.nips.cc/paper/5957-mind-the-gap-a-generative-approach-to-interpretable-feature-selection-and-extraction.pdf},
 pages = {2260--2268},
 publisher = {{Curran Associates, Inc}},
 editor = {{C. Cortes} and {N. D. Lawrence} and {D. D. Lee} and {M. Sugiyama} and {R. Garnett}},
 booktitle = {Advances in Neural Information Processing Systems 28},
 year = {2015}
}


@article{Kindermans.2017,
 author = {Kindermans, Pieter-Jan and Sch{\"u}tt, Kristof T. and Alber, Maximilian and M{\"u}ller, Klaus-Robert and D{\"a}hne, Sven},
 year = {2017},
 title = {PatternNet and PatternLRP--improving the interpretability of neural networks},
 pages = {16},
 volume = {1050},
 journal = {stat}
}


@article{Kindermans.2017b,
 author = {Kindermans, Pieter-Jan and Sch{\"u}tt, Kristof T. and Alber, Maximilian and M{\"u}ller, Klaus-Robert and Erhan, Dumitru and Kim, Been and D{\"a}hne, Sven},
 year = {2017},
 title = {Learning how to explain neural networks: PatternNet and PatternAttribution},
 journal = {arXiv preprint arXiv:1705.05598}
}


@article{Koh.2017,
 author = {Koh, Pang Wei and Liang, Percy},
 year = {2017},
 title = {Understanding black-box predictions via influence functions},
 keywords = {indirect influence},
 journal = {arXiv preprint arXiv:1703.04730}
}


@article{Krause.2017,
 author = {Krause, Josua and Dasgupta, Aritra and Swartz, Jordan and Aphinyanaphongs, Yindalon and Bertini, Enrico},
 year = {2017},
 title = {A Workflow for Visual Diagnostics of Binary Classifiers using Instance-Level Explanations},
 journal = {arXiv preprint arXiv:1705.01968}
}


@article{Hiilldobler.1990,
 author = {Hiilldobler, Steffen and Edu, Ema Steffenqicsi Berkeley},
 year = {1990},
 title = {A structured connectionist unification algorithm},
 journal = {Proceedings of the National Conference on Artificial Intelligence AAAI90}
}


@article{Ma.2018,
 author = {Ma, Jianzhu and Yu, Michael Ku and Fong, Samson and Ono, Keiichiro and Sage, Eric and Demchak, Barry and Sharan, Roded and Ideker, Trey},
 year = {2018},
 title = {Using deep learning to model the hierarchical structure and function of a cell},
 pages = {290},
 volume = {15},
 number = {4},
 journal = {Nature methods}
}


@article{Marblestone.2016,
 abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
 author = {Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
 year = {2016},
 title = {Toward an Integration of Deep Learning and Neuroscience},
 url = {https://www.frontiersin.org/article/10.3389/fncom.2016.00094},
 pages = {94},
 volume = {10},
 issn = {1662-5188},
 journal = {Frontiers in Computational Neuroscience},
 doi = {10.3389/fncom.2016.00094}
}


@inproceedings{Shtern.2017,
 author = {Shtern, Mark and Ejaz, Rabia and Tzerpos, Vassilios},
 title = {Transfer Learning in Neural Networks: An Experience Report},
 url = {http://dl.acm.org/citation.cfm?id=3172795.3172818},
 keywords = {deep learning;music classification;Transfer learning},
 pages = {201--210},
 publisher = {{IBM Corp}},
 series = {CASCON '17},
 booktitle = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
 year = {2017},
 address = {Riverton, NJ, USA}
}


@article{Si.2013,
 author = {Si, Zhangzhang and Zhu, Song-Chun},
 year = {2013},
 title = {Learning and-or templates for object recognition and detection},
 pages = {2189--2205},
 volume = {35},
 number = {9},
 journal = {IEEE transactions on pattern analysis and machine intelligence}
}


@article{Simonyan.2013,
 author = {Simonyan, Karen and Vedaldi, Andrea and Zisserman, Andrew},
 year = {2013},
 title = {Deep inside convolutional networks: Visualising image classification models and saliency maps},
 journal = {arXiv preprint arXiv:1312.6034}
}


@proceedings{Springer.2016,
 year = {2016},
 title = {Conference of the Italian Association for Artificial Intelligence},
 institution = {Springer}
}


@proceedings{Springer.2016b,
 year = {2016},
 title = {European Conference on Computer Vision},
 institution = {Springer}
}


@article{Strobelt.2018,
 author = {Strobelt, Hendrik and Gehrmann, Sebastian and Behrisch, Michael and Perer, Adam and Pfister, Hanspeter and Rush, Alexander M.},
 year = {2018},
 title = {Seq2Seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models},
 journal = {arXiv preprint arXiv:1804.09299}
}


@inproceedings{Talbot.2009,
 author = {Talbot, Justin and Lee, Bongshin and Kapoor, Ashish and Tan, Desney S.},
 title = {EnsembleMatrix: Interactive Visualization to Support Machine Learning with Multiple Classifiers},
 url = {http://doi.acm.org/10.1145/1518701.1518895},
 keywords = {caltech-101;ensemble classifiers;interactive machine learning;object recognition;visualization},
 pages = {1283--1292},
 publisher = {ACM},
 isbn = {978-1-60558-246-7},
 series = {CHI '09},
 booktitle = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 year = {2009},
 address = {New York, NY, USA},
 doi = {10.1145/1518701.1518895}
}


@inproceedings{Tamagnini.2017,
 author = {Tamagnini, Paolo and Krause, Josua and Dasgupta, Aritra and Bertini, Enrico},
 title = {Interpreting Black-Box Classifiers Using Instance-Level Visual Explanations},
 url = {http://doi.acm.org/10.1145/3077257.3077260},
 keywords = {classification;explanation;Machine learning;visual analytics},
 pages = {6:1--6:6},
 publisher = {ACM},
 isbn = {978-1-4503-5029-7},
 series = {HILDA'17},
 booktitle = {Proceedings of the 2Nd Workshop on Human-In-the-Loop Data Analytics},
 year = {2017},
 address = {New York, NY, USA},
 doi = {10.1145/3077257.3077260}
}


@incollection{Tan.,
 author = {Tan, Sahra and Caruana, Rich and Hooker, Giles and Lou, Yin},
 title = {Auditing Black-Box Models Using~Transparent Model Distillation With Side Information},
 url = {https://arxiv.org/html/1711.09889},
 booktitle = {Proceedings of the  NIPS 2017 Symposium on Interpretable Machine Learning}
}


@misc{Taylor.2009,
 author = {Taylor, Matthew E. and Stone, Peter},
 year = {2009},
 title = {Transfer Learning for Reinforcement Learning Domains: A Survey},
 url = {http://dl.acm.org/citation.cfm?id=1577069.1755839},
 keywords = {Reinforcement Learning},
 volume = {10},
 publisher = {JMLR.org},
 journal = {J. Mach. Learn. Res.}
}


@article{TianqiChen.2015,
 author = {{Tianqi Chen} and {Ian J. Goodfellow} and {Jonathon Shlens}},
 year = {2015},
 title = {Net2Net: Accelerating Learning via Knowledge Transfer},
 volume = {abs/1511.05641},
 journal = {CoRR}
}


@article{Towell.1994,
 author = {Towell, Geoffrey G. and Shavlik, Jude W.},
 year = {1994},
 title = {Knowledge-based artificial neural networks},
 pages = {119--165},
 volume = {70},
 number = {1-2},
 issn = {0004-3702},
 journal = {Artificial Intelligence}
}


@book{vanHarmelen.2008,
 author = {{van Harmelen}, Frank and Lifschitz, Vladimir and Porter, Bruce},
 year = {2008},
 title = {Handbook of knowledge representation},
 volume = {1},
 publisher = {Elsevier}
}


@book{Wiering.2012,
 year = {2012},
 title = {Reinforcement Learning: State-of-the-Art},
 address = {Berlin, Heidelberg},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-27645-3},
 editor = {Wiering, Marco and {van Otterlo}, Martijn}
}


@inproceedings{Yang.2015,
 author = {Yang, Shuo and Luo, Ping and Loy, Chen-Change and Tang, Xiaoou},
 title = {From Facial Parts Responses to Face Detection: A Deep Learning Approach},
 booktitle = {The IEEE International Conference on Computer Vision (ICCV)},
 year = {2015}
}


@inproceedings{Serafini.2016,
 author = {Serafini, Luciano and Garcez, Artur S. Avila},
 title = {Learning and reasoning with logic tensor networks},
 keywords = {Logic Tensor Networks},
 pages = {334--348},
 booktitle = {Conference of the Italian Association for Artificial Intelligence},
 year = {2016}
}


@article{Manhaeve.2018,
 author = {Manhaeve, Robin and Duman{\v{c}}i{\'c}, Sebastijan and Kimmig, Angelika and Demeester, Thomas and de Raedt, Luc},
 year = {2018},
 title = {DeepProbLog: Neural Probabilistic Logic Programming},
 journal = {arXiv preprint arXiv:1805.10872}
}


@article{Samek.2017,
 author = {Samek, Wojciech and Binder, Alexander and Montavon, Gr{\'e}goire and Lapuschkin, Sebastian and M{\"u}ller, Klaus-Robert},
 year = {2017},
 title = {Evaluating the visualization of what a deep neural network has learned},
 pages = {2660--2673},
 volume = {28},
 number = {11},
 journal = {IEEE transactions on neural networks and learning systems}
}


@article{Ross.2017,
 author = {Ross, Andrew Slavin and Hughes, Michael C. and Doshi-Velez, Finale},
 year = {2017},
 title = {Right for the right reasons: Training differentiable models by constraining their explanations},
 keywords = {GDPR;right to explanation},
 journal = {arXiv preprint arXiv:1703.03717}
}


@article{MartinMozina.2007,
 abstract = {We present a novel approach to machine learning, called ABML (argumentation based ML). This approach combines machine learning from examples with concepts from the field of argumentation. The idea is to provide expert's arguments, or reasons, for some of the learning examples. We require that the theory induced from the examples explains the examples in terms of the given reasons. Thus arguments constrain the combinatorial search among possible hypotheses, and also direct the search towards hypotheses that are more comprehensible in the light of expert's background knowledge. In this paper we realize the idea of ABML as rule learning. We implement ABCN2, an argument-based extension of the CN2 rule learning algorithm, conduct experiments and analyze its performance in comparison with the original CN2 algorithm.},
 author = {{Martin Mo{\v{z}}ina} and {Jure {\v{Z}}abkar} and {Ivan Bratko}},
 year = {2007},
 title = {Argument based machine learning},
 url = {http://www.sciencedirect.com/science/article/pii/S0004370207000690},
 keywords = {Argumentation;Background knowledge;Knowledge intensive learning;Learning through arguments;Machine learning},
 pages = {922--937},
 volume = {171},
 number = {10},
 issn = {0004-3702},
 journal = {Artificial Intelligence},
 doi = {10.1016/j.artint.2007.04.007}
}


@inproceedings{MdKamruzzamanSarkerNingXieDerekDoranMichaelRaymerPascalHitzler.2017,
 author = {{Md Kamruzzaman Sarker, Ning Xie, Derek Doran, Michael Raymer, Pascal Hitzler}},
 title = {Explaining Trained Neural Networks with Semantic Web Technologies: First Steps},
 keywords = {semantic web},
 volume = {17},
 booktitle = {Proceedings of the Twelfth International Workshop on Neural-Symbolic Learning and Reasoning, NeSy},
 year = {2017}
}


@article{Michalski.2000,
 abstract = {A new class of evolutionary computation processes is presented, called Learnable Evolution Model or LEM. In contrast to Darwinian-type evolution that relies on mutation, recombination, and selection operators, LEM employs machine learning to generate new populations. Specifically, in Machine Learning mode, a learning system seeks reasons why certain individuals in a population (or a collection of past populations) are superior to others in performing a designated class of tasks. These reasons, expressed as inductive hypotheses, are used to generate new populations. A remarkable property of LEM is that it is capable of quantum leaps (``insight jumps'') of the fitness function, unlike Darwinian-type evolution that typically proceeds through numerous slight improvements. In our early experimental studies, LEM significantly outperformed evolutionary computation methods used in the experiments, sometimes achieving speed-ups of two or more orders of magnitude in terms of the number of evolutionary steps. LEM has a potential for a wide range of applications, in particular, in such domains as complex optimization or search problems, engineering design, drug design, evolvable hardware, software engineering, economics, data mining, and automatic programming.},
 author = {Michalski, Ryszard S.},
 year = {2000},
 title = {LEARNABLE EVOLUTION MODEL: Evolutionary Processes Guided by Machine Learning},
 pages = {9--40},
 volume = {38},
 number = {1},
 issn = {1573-0565},
 journal = {Machine Learning},
 doi = {10.1023/A:1007677805582}
}


@inproceedings{Montone.2015,
 author = {Montone, Guglielmo and O'Regan, J. Kevin and Terekhov, Alexander V.},
 title = {The usefulness of past knowledge when learning a new task in deep neural networks},
 pages = {10--18},
 booktitle = {Proceedings of the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches-Volume 1583},
 year = {2015}
}


@article{Muggleton.2018,
 abstract = {Statistical machine learning is widely used in image classification. However, most techniques (1) require many images to achieve high accuracy and (2) do not provide support for reasoning below the level of classification, and so are unable to support secondary reasoning, such as the existence and position of light sources and other objects outside the image. This paper describes an Inductive Logic Programming approach called Logical Vision which overcomes some of these limitations. LV uses Meta-Interpretive Learning (MIL) combined with low-level extraction of high-contrast points sampled from the image to learn recursive logic programs describing the image. In published work LV was demonstrated capable of high-accuracy prediction of classes such as regular polygon from small numbers of images where Support Vector Machines and Convolutional Neural Networks gave near random predictions in some cases. LV has so far only been applied to noise-free, artificially generated images. This paper extends LV by (a) addressing classification noise using a new noise-telerant version of the MIL system Metagol, (b) addressing attribute noise using primitive-level statistical estimators to identify sub-objects in real images, (c) using a wider class of background models representing classical 2D shapes such as circles and ellipses, (d) providing richer learnable background knowledge in the form of a simple but generic recursive theory of light reflection. In our experiments we consider noisy images in both natural science settings and in a RoboCup competition setting. The natural science settings involve identification of the position of the light source in telescopic and microscopic images, while the RoboCup setting involves identification of the position of the ball. Our results indicate that with real images the new noise-robust version of LV using a single example (i.e. one-shot LV) converges to an accuracy at least comparable to a thirty-shot statistical machine learner on both prediction of hidden light sources in the scientific settings and in the RoboCup setting. Moreover, we demonstrate that a general background recursive theory of light can itself be invented using LV and used to identify ambiguities in the convexity/concavity of objects such as craters in the scientific setting and partial obscuration of the ball in the RoboCup setting.},
 author = {Muggleton, Stephen and Dai, Wang-Zhou and Sammut, Claude and Tamaddoni-Nezhad, Alireza and Wen, Jing and Zhou, Zhi-Hua},
 year = {2018},
 title = {Meta-Interpretive Learning from noisy images},
 issn = {1573-0565},
 journal = {Machine Learning},
 doi = {10.1007/s10994-018-5710-8}
}


@article{Muggleton.2018b,
 abstract = {During the 1980s Michie defined Machine Learning in terms of two orthogonal axes of performance: predictive accuracy and comprehensibility of generated hypotheses. Since predictive accuracy was readily measurable and comprehensibility not so, later definitions in the 1990s, such as Mitchell's, tended to use a one-dimensional approach to Machine Learning based solely on predictive accuracy, ultimately favouring statistical over symbolic Machine Learning approaches. In this paper we provide a definition of comprehensibility of hypotheses which can be estimated using human participant trials. We present two sets of experiments testing human comprehensibility of logic programs. In the first experiment we test human comprehensibility with and without predicate invention. Results indicate comprehensibility is affected not only by the complexity of the presented program but also by the existence of anonymous predicate symbols. In the second experiment we directly test whether any state-of-the-art ILP systems are ultra-strong learners in Michie's sense, and select the Metagol system for use in humans trials. Results show participants were not able to learn the relational concept on their own from a set of examples but they were able to apply the relational definition provided by the ILP system correctly. This implies the existence of a class of relational concepts which are hard to acquire for humans, though easy to understand given an abstract explanation. We believe improved understanding of this class could have potential relevance to contexts involving human learning, teaching and verbal interaction.},
 author = {Muggleton, Stephen H. and Schmid, Ute and Zeller, Christina and Tamaddoni-Nezhad, Alireza and Besold, Tarek R.},
 year = {2018},
 title = {Ultra-Strong Machine Learning: comprehensibility of programs learned with ILP},
 issn = {1573-0565},
 journal = {Machine Learning},
 doi = {10.1007/s10994-018-5707-3}
}


@inproceedings{Narvekar.2016,
 author = {Narvekar, Sanmit and Sinapov, Jivko and Leonetti, Matteo and Stone, Peter},
 title = {Source Task Creation for Curriculum Learning},
 url = {http://dl.acm.org/citation.cfm?id=2936924.2937007},
 keywords = {curriculum learning;Reinforcement Learning;Transfer learning},
 pages = {566--574},
 publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
 isbn = {978-1-4503-4239-1},
 series = {AAMAS '16},
 booktitle = {Proceedings of the 2016 International Conference on Autonomous Agents {\&} Multiagent Systems},
 year = {2016},
 address = {Richland, SC}
}


@inproceedings{Patel.2010,
 author = {Patel, Kayur and Bancroft, Naomi and Drucker, Steven M. and Fogarty, James and Ko, Andrew J. and Landay, James},
 title = {Gestalt: Integrated Support for Implementation and Analysis in Machine Learning},
 url = {http://doi.acm.org/10.1145/1866029.1866038},
 keywords = {gestalt;Machine learning;software development},
 pages = {37--46},
 publisher = {ACM},
 isbn = {978-1-4503-0271-5},
 series = {UIST '10},
 booktitle = {Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology},
 year = {2010},
 address = {New York, NY, USA},
 doi = {10.1145/1866029.1866038}
}


@inproceedings{Patel.2011,
 author = {Patel, Kayur and Drucker, Steven M. and Fogarty, James and Kapoor, Ashish and Tan, Desney S.},
 title = {Using multiple models to understand data},
 pages = {1723},
 volume = {22},
 booktitle = {IJCAI Proceedings-International Joint Conference on Artificial Intelligence},
 year = {2011}
}


@inproceedings{Pedreshi.2008,
 author = {Pedreshi, Dino and Ruggieri, Salvatore and Turini, Franco},
 title = {Discrimination-aware data mining},
 pages = {560--568},
 booktitle = {Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
 year = {2008}
}


@article{Preece.2018,
 author = {Preece, Alun},
 year = {2018},
 title = {Asking `Why'in AI: Explainability of intelligent systems--perspectives and challenges},
 pages = {63--72},
 volume = {25},
 number = {2},
 journal = {Intelligent Systems in Accounting, Finance and Management}
}


@inproceedings{Punjabi.2017,
 author = {Punjabi, Arjun and Katsaggelos, Aggelos K.},
 title = {Visualization of feature evolution during convolutional neural network training},
 keywords = {activation maximization},
 pages = {311--315},
 booktitle = {Signal Processing Conference (EUSIPCO), 2017 25th European},
 year = {2017}
}


@inproceedings{Ribeiro.2016,
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 title = {Why should i trust you?: Explaining the predictions of any classifier},
 pages = {1135--1144},
 booktitle = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 year = {2016}
}


@inproceedings{Ribeiro.2018,
 author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
 title = {Anchors: High-Precision Model-Agnostic Explanations},
 year = {2018}
}


@article{Rokach.2010,
 abstract = {The idea of ensemble methodology is to build a predictive model by integrating multiple models. It is well-known that ensemble methods can be used for improving prediction performance. Researchers from various disciplines such as statistics and AI considered the use of ensemble methodology. This paper, review existing ensemble techniques and can be served as a tutorial for practitioners who are interested in building ensemble based systems.},
 author = {Rokach, Lior},
 year = {2010},
 title = {Ensemble-based classifiers},
 keywords = {ensemble},
 pages = {1--39},
 volume = {33},
 number = {1},
 issn = {1573-7462},
 journal = {Artificial Intelligence Review},
 doi = {10.1007/s10462-009-9124-7}
}


@article{S.Krening.2017,
 author = {{S. Krening} and {B. Harrison} and {K. M. Feigh} and {C. L. Isbell} and {M. Riedl} and {A. Thomaz}},
 year = {2017},
 title = {Learning From Explanations Using Sentiment and Advice in RL},
 keywords = {adversarial advice;Advice;false negative mitigation;Games;intelligent robots;learning (artificial intelligence);Machine learning;Mario Bros. game;object-focused advice;Reinforcement Learning;robot learning;Robots;sentiment analysis;software agent;Vocabulary;warnings},
 pages = {44--55},
 volume = {9},
 number = {1},
 issn = {2379-8920},
 journal = {IEEE Transactions on Cognitive and Developmental Systems},
 doi = {10.1109/TCDS.2016.2628365}
}


@inproceedings{Z.Tang.2016,
 author = {{Z. Tang} and {D. Wang} and {Z. Zhang}},
 title = {Recurrent neural network training with dark knowledge transfer},
 keywords = {automatic speech recognition;child models;deep neural network model;Hafnium;Knowledge transfer;knowledge transfer learning;learning (artificial intelligence);long short-term memory;neural nets;Predictive models;recurrent neural nets;recurrent neural network;recurrent neural network training;Recurrent neural networks;speech recognition;teacher model;Training;Training data},
 pages = {5900--5904},
 booktitle = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
 year = {2016},
 doi = {10.1109/ICASSP.2016.7472809}
}


@inproceedings{Hendricks.2016,
 author = {Hendricks, Lisa Anne and Akata, Zeynep and Rohrbach, Marcus and Donahue, Jeff and Schiele, Bernt and Darrell, Trevor},
 title = {Generating visual explanations},
 pages = {3--19},
 booktitle = {European Conference on Computer Vision},
 year = {2016}
}


@article{Gulcehre.2016,
 author = {G{\"u}l{\c{c}}ehre, {\c{C}}aǧlar and Bengio, Yoshua},
 year = {2016},
 title = {Knowledge matters: Importance of prior information for optimization},
 keywords = {Background knowledge},
 pages = {226--257},
 volume = {17},
 number = {1},
 journal = {The Journal of Machine Learning Research}
}


@proceedings{.2016,
 year = {2016},
 title = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-3362-7},
 series = {CHI '16}
}


@proceedings{.2016b,
 year = {2016},
 title = {Proceedings of the 2016 CHI Conference on Human Factors in Computing Systems},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-3362-7},
 series = {CHI '16}
}


@proceedings{.2016c,
 year = {2016},
 title = {Proceedings of the 2016 International Conference on Autonomous Agents {\&} Multiagent Systems},
 address = {Richland, SC},
 publisher = {{International Foundation for Autonomous Agents and Multiagent Systems}},
 isbn = {978-1-4503-4239-1},
 series = {AAMAS '16}
}


@proceedings{.2016d,
 year = {2016},
 title = {Proceedings of the 31st IEEE/ACM International Conference on Automated Software Engineering},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-3845-5},
 series = {ASE 2016}
}


@proceedings{.2016e,
 year = {2016},
 title = {Proceedings of the 54th Annual Meeting of the Association for      Computational Linguistics (Volume 1: Long Papers)},
 publisher = {{Association for Computational Linguistics}}
}


@proceedings{.2017,
 year = {2017},
 title = {IEEE Computer Vision and Pattern Recognition (CVPR) Workshop}
}


@proceedings{.2017b,
 year = {2017},
 title = {IEEE Smart World Congress 2017 Workshop: DAIS}
}


@proceedings{.2017c,
 year = {2017},
 title = {Proceedings of the 16th Edition of the International Conference on Articial Intelligence and Law},
 keywords = {GDPR},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-4891-1},
 series = {ICAIL '17}
}


@proceedings{.2017d,
 year = {2017},
 title = {Proceedings of the 27th Annual International Conference on Computer Science and Software Engineering},
 address = {Riverton, NJ, USA},
 publisher = {{IBM Corp}},
 series = {CASCON '17}
}


@proceedings{.2017e,
 year = {2017},
 title = {Proceedings of the 2Nd Workshop on Human-In-the-Loop Data Analytics},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-5029-7},
 series = {HILDA'17}
}


@proceedings{.2017f,
 year = {2017},
 title = {Proceedings of the Twelfth International Workshop on Neural-Symbolic Learning and Reasoning, NeSy}
}


@proceedings{AAAI.2018,
 year = {2018},
 institution = {AAAI}
}


@proceedings{ACM.2008,
 year = {2008},
 title = {Proceedings of the 14th ACM SIGKDD international conference on Knowledge discovery and data mining},
 institution = {ACM}
}


@proceedings{ACM.2016,
 year = {2016},
 title = {Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 institution = {ACM}
}


@proceedings{ACM.2016b,
 year = {2016},
 title = {Proceedings of the Workshop on Human-In-the-Loop Data Analytics},
 institution = {ACM}
}


@proceedings{.2016f,
 year = {2016},
 title = {Advances in neural information processing systems}
}


@proceedings{ACM.2017,
 year = {2017},
 title = {Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 keywords = {Selective Labels Problem;Unobservables},
 institution = {ACM}
}


@proceedings{.2016g,
 year = {2016},
 title = {Advances in neural information processing systems}
}


@proceedings{.2015,
 year = {2015},
 title = {The IEEE International Conference on Computer Vision (ICCV)}
}


@book{.b,
 title = {Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on}
}


@book{.c,
 title = {Conference on Artificial Intelligence, Workshop on Affective Content Analysis}
}


@proceedings{.d,
 title = {IJCAI-17 Workshop on Explainable AI (XAI)}
}


@book{.e,
 title = {Proceedings of the  NIPS 2017 Symposium on Interpretable Machine Learning}
}


@book{.f,
 title = {The AAAI-18 Workshop on Affective Content Analysis (AFFCON2018)}
}


@proceedings{.2008,
 year = {2008},
 title = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-60558-193-4},
 series = {KDD '08}
}


@proceedings{.2008b,
 year = {2008},
 title = {Proceedings of the 17th ACM Conference on Information and Knowledge Management},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-59593-991-3},
 series = {CIKM '08}
}


@proceedings{.2009,
 year = {2009},
 title = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-60558-512-3},
 series = {CIKM '09}
}


@proceedings{.2009b,
 year = {2009},
 title = {Proceedings of the 26th Annual International Conference on Machine Learning},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-60558-516-1},
 series = {ICML '09}
}


@proceedings{.2009c,
 year = {2009},
 title = {Proceedings of the SIGCHI Conference on Human Factors in Computing Systems},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-60558-246-7},
 series = {CHI '09}
}


@proceedings{.2010,
 year = {2010},
 title = {Proceedings of the 23Nd Annual ACM Symposium on User Interface Software and Technology},
 address = {New York, NY, USA},
 publisher = {ACM},
 isbn = {978-1-4503-0271-5},
 series = {UIST '10}
}


@proceedings{.2011,
 year = {2011},
 title = {IJCAI Proceedings-International Joint Conference on Artificial Intelligence}
}


@proceedings{.2015b,
 year = {2015},
 title = {Advances in neural information processing systems}
}


@proceedings{.2015c,
 year = {2015},
 title = {CoCo@ NIPS}
}


@proceedings{.2015d,
 year = {2015},
 title = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}
}


@proceedings{.2016h,
 year = {2016},
 title = {2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}
}


@inproceedings{He.2009,
 author = {He, Jingrui and Liu, Yan and Lawrence, Richard},
 title = {Graph-based Transfer Learning},
 url = {http://doi.acm.org/10.1145/1645953.1646073},
 keywords = {graph-based;Transfer learning},
 pages = {937--946},
 publisher = {ACM},
 isbn = {978-1-60558-512-3},
 series = {CIKM '09},
 booktitle = {Proceedings of the 18th ACM Conference on Information and Knowledge Management},
 year = {2009},
 address = {New York, NY, USA},
 doi = {10.1145/1645953.1646073}
}


@article{Adler.2018,
 author = {Adler, Philip and Falk, Casey and Friedler, Sorelle A. and Nix, Tionney and Rybeck, Gabriel and Scheidegger, Carlos and Smith, Brandon and Venkatasubramanian, Suresh},
 year = {2018},
 title = {Auditing black-box models for indirect influence},
 keywords = {indirect influence},
 pages = {95--122},
 volume = {54},
 number = {1},
 journal = {Knowledge and Information Systems}
}


@article{AlvarezMelis.2018,
 author = {Alvarez-Melis, David and Jaakkola, Tommi S.},
 year = {2018},
 title = {Towards Robust Interpretability with Self-Explaining Neural Networks},
 journal = {arXiv preprint arXiv:1806.07538}
}


@inproceedings{Gao.2008,
 author = {Gao, Jing and Fan, Wei and Jiang, Jing and Han, Jiawei},
 title = {Knowledge Transfer via Multiple Model Local Structure Mapping},
 url = {http://doi.acm.org/10.1145/1401890.1401928},
 keywords = {classification;ensemble;semi-supervised learning;Transfer learning},
 pages = {283--291},
 publisher = {ACM},
 isbn = {978-1-60558-193-4},
 series = {KDD '08},
 booktitle = {Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining},
 year = {2008},
 address = {New York, NY, USA},
 doi = {10.1145/1401890.1401928}
}


@article{Garcez.2001,
 author = {Garcez, Artur S. Avila and Broda, Krysia and Gabbay, Dov M.},
 year = {2001},
 title = {Symbolic knowledge extraction from trained neural networks: A sound approach},
 keywords = {knowledge extraction},
 pages = {155--207},
 volume = {125},
 number = {1-2},
 issn = {0004-3702},
 journal = {Artificial Intelligence}
}


@article{Garcez.2001b,
 author = {Garcez, Artur S. Avila and {K Broda} and {D.M Gabbay}},
 year = {2001},
 title = {Symbolic knowledge extraction from trained neural networks: A sound approach},
 url = {http://www.sciencedirect.com/science/article/pii/S0004370200000771},
 keywords = {Artificial neural networks;knowledge extraction;Neural-symbolic integration;Nonmonotonic reasoning;Rule extraction},
 pages = {155--207},
 volume = {125},
 number = {1},
 issn = {0004-3702},
 journal = {Artificial Intelligence},
 doi = {10.1016/S0004-3702(00)00077-1}
}


@book{Garcez.2008,
 author = {Garcez, Artur S. Avila and Lamb, Luis C. and Gabbay, Dov M.},
 year = {2008},
 title = {Neural-symbolic cognitive reasoning},
 publisher = {{Springer Science {\&} Business Media}},
 isbn = {978-3-540-73245-7}
}


@incollection{Garcez.,
 author = {Garcez, Artur S. Avila and Serafini, L.},
 title = {Logic Tensor Networks: Deep Learning and Logical Reasoning from Data and Knowledge.},
 keywords = {Logic Tensor Networks},
 booktitle = {1th International Workshop on Neural-Symbolic Learning and Reasoning (NeSy16)}
}


@article{Garcez.1999,
 author = {Garcez, Artur S. Avila and Zaverucha, Gerson},
 year = {1999},
 title = {The connectionist inductive learning and logic programming system},
 keywords = {CILP;DNA sequence analysis},
 pages = {59--77},
 volume = {11},
 number = {1},
 issn = {1573-7497},
 journal = {Applied Intelligence}
}


@inproceedings{Garcez.1997,
 author = {Garcez, Artur S. Avila and Zaverucha, Gerson and {da Silva, Victor Navarro AL}},
 title = {Applying the connectionist inductive learning and logic programming system to power system diagnosis},
 keywords = {CILP},
 pages = {121--126},
 volume = {1},
 booktitle = {Neural Networks, 1997., International Conference on},
 year = {1997}
}


@article{Ghosh.2018,
 author = {Ghosh, Shalini and Mercier, Amaury and Pichapati, Dheeraj and Jha, Susmit and Yegneswaran, Vinod and Lincoln, Patrick},
 year = {2018},
 title = {Trusted Neural Networks for Safety-Constrained Autonomous Control},
 keywords = {safety},
 journal = {arXiv preprint arXiv:1805.07075}
}


@book{Goertzel.2008,
 author = {Goertzel, Ben and Ikl{\'e}, Matthew and Goertzel, Izabela Freire and Heljakka, Ari},
 year = {2008},
 title = {Probabilistic logic networks: A comprehensive framework for uncertain inference},
 keywords = {Probabilistic Logic Networks},
 publisher = {{Springer Science {\&} Business Media}}
}


@inproceedings{Goertzel.2013,
 author = {Goertzel, Ben and Ke, Shujing and Lian, Ruiting and O'Neill, Jade and Sadeghi, Keyvan and Wang, Dingjie and Watkins, Oliver and Yu, Gino},
 title = {The cogprime architecture for embodied artificial general intelligence},
 keywords = {general intelligence;opencog},
 pages = {60--67},
 booktitle = {Computational Intelligence for Human-like Intelligence (CIHLI), 2013 IEEE Symposium on},
 year = {2013}
}


@book{Goertzel.2014,
 author = {Goertzel, Ben and Pennachin, Cassio and Geisweiller, Nil},
 year = {2014},
 title = {Engineering General Intelligence, Part 1: A Path to Advanced AGI via Embodied Learning and Cognitive Synergy},
 keywords = {general intelligence;opencog},
 publisher = {{Atlantis Publishing Corporation}},
 isbn = {9462390266}
}


@book{Goertzel.2014b,
 author = {Goertzel, Ben and Pennachin, Cassio and Geisweiller, Nil},
 year = {2014},
 title = {Engineering General Intelligence, Part 2: The CogPrime Architecture for Integrative, Embodied AGI},
 keywords = {general intelligence;opencog},
 publisher = {{Atlantis Publishing Corporation}},
 isbn = {9462390290}
}


@inproceedings{Goertzel.2013b,
 abstract = {The bridging of the gap between 1) subsymbolic pattern recognition and learning algorithms and 2) symbolic reasoning algorithms, has been a major issue for AI since the early days of the field. One class of approaches involves integrating subsymbolic and symbolic systems, but this raises the question of how to effectively translate between the very different languages involved. In the approach described here, a frequent subtree mining algorithm is used to identify recurrent patterns in the state of a hierarchical deep learning system (DeSTIN) that is exposed to visual stimuli. The relationships between state-subtrees and percepts are then input to a probabilistic logic system (OpenCog's Probabilistic Logic Networks), which conducts uncertain inferences using them as axioms. The core conceptual idea is to use patterns in the states inferred by a perceptual hierarchy, as inputs to an uncertain logic system. Simple illustrative examples are presented based on the presentation of images of typed letters to DeSTIN. This work forms a component of a larger project to integrate perceptual, motoric and cognitive processing within the integrative OpenCog cognitive architecture.},
 author = {Goertzel, Ben and Sanders, Ted and O'Neill, Jade},
 title = {Integrating Deep Learning Based Perception with Probabilistic Logic via Frequent Pattern Mining},
 keywords = {frequent pattern mining},
 pages = {40--49},
 publisher = {{Springer Berlin Heidelberg}},
 isbn = {978-3-642-39521-5},
 editor = {K{\"u}hnberger, Kai-Uwe and Rudolph, Sebastian and Wang, Pei},
 booktitle = {Artificial General Intelligence},
 year = {2013},
 address = {Berlin, Heidelberg}
}


@article{Goodman.2016,
 author = {Goodman, Bryce and Flaxman, Seth},
 year = {2016},
 title = {European Union regulations on algorithmic decision-making and a{\textquotedbl} right to explanation},
 keywords = {GDPR},
 journal = {arXiv preprint arXiv:1606.08813}
}


@article{Guidotti.2018,
 author = {Guidotti, Riccardo and Monreale, Anna and Turini, Franco and Pedreschi, Dino and Giannotti, Fosca},
 year = {2018},
 title = {A survey of methods for explaining black box models},
 keywords = {literature review},
 journal = {arXiv preprint arXiv:1802.01933}
}


@article{Freitas.2014,
 author = {Freitas, Alex A.},
 year = {2014},
 title = {Comprehensible Classification Models: A Position Paper},
 url = {http://doi.acm.org/10.1145/2594473.2594475},
 keywords = {Bayesian network classifiers;decision table;decision tree;monotonicity constraint;nearest neighbors;rule induction},
 pages = {1--10},
 volume = {15},
 number = {1},
 issn = {1931-0145},
 journal = {SIGKDD Explor. Newsl.},
 doi = {10.1145/2594473.2594475}
}


@article{AleksandarChakarov.2016,
 author = {{Aleksandar Chakarov} and {Aditya V. Nori} and {Sriram K. Rajamani} and {Shayak Sen} and {Deepak Vijaykeerthy}},
 year = {2016},
 title = {Debugging Machine Learning Tasks},
 volume = {abs/1603.07292},
 journal = {CoRR}
}


@inproceedings{Franca.2015,
 author = {Fran{\c{c}}a, Manoel Vitor Macedo and Garcez, Artur S. Avila and Zaverucha, Gerson},
 title = {Relational Knowledge Extraction from Neural Networks},
 keywords = {knowledge extraction},
 booktitle = {CoCo@ NIPS},
 year = {2015}
}


@inproceedings{Duivesteijn.2014,
 author = {Duivesteijn, Wouter and Thaele, Julia},
 title = {Understanding where your classifier does (not) work--the SCaPE model class for EMM},
 pages = {809--814},
 booktitle = {2014 IEEE International Conference on Data Mining (ICDM)},
 year = {2014}
}


@article{AnestisFachantidis.2013,
 author = {{Anestis Fachantidis} and {Ioannis Partalas} and {Grigorios Tsoumakas} and {Ioannis Vlahavas}},
 year = {2013},
 title = {Transferring task models in Reinforcement Learning agents},
 url = {http://www.sciencedirect.com/science/article/pii/S0925231212007771},
 keywords = {Model-based Reinforcement Learning;Reinforcement Learning;Transfer learning},
 pages = {23--32},
 volume = {107},
 issn = {0925-2312},
 journal = {Neurocomputing},
 doi = {10.1016/j.neucom.2012.08.039}
}


@article{AnhMaiNguyenandJasonYosinskiandJeffClune.2015,
 author = {{Anh Mai Nguyen and Jason Yosinski and Jeff Clune}},
 year = {2015},
 title = {Deep Neural Networks are Easily Fooled: High Deep Neural Networks Are Easily Fooled :~High Confidence Predictions for Unrecognizable Images},
 pages = {427--436},
 journal = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition}
}


@article{Bach.2015,
 author = {Bach, Sebastian and Binder, Alexander and Montavon, Gr{\'e}goire and Klauschen, Frederick and M{\"u}ller, Klaus-Robert and Samek, Wojciech},
 year = {2015},
 title = {On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation},
 keywords = {heatmaps;pixel wise decomposition},
 pages = {e0130140},
 volume = {10},
 number = {7},
 journal = {PloS one}
}


@article{Baehrens.2010,
 author = {Baehrens, David and Schroeter, Timon and Harmeling, Stefan and Kawanabe, Motoaki and Hansen, Katja and M{\~A}{\v{z}}ller, Klaus-Robert},
 year = {2010},
 title = {How to explain individual classification decisions},
 pages = {1803--1831},
 volume = {11},
 number = {Jun},
 journal = {Journal of Machine Learning Research}
}


@incollection{BauDavidandZhouBoleiandKhoslaAdityaandOlivaAudeandTorralbaAntonio.,
 author = {{Bau, David and Zhou, Bolei and Khosla, Aditya and Oliva, Aude and Torralba, Antonio}},
 title = {Network dissection: Quantifying interpretability of deep visual representations},
 pages = {3319--3327},
 booktitle = {Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on}
}


@inproceedings{Bengio.2009,
 author = {Bengio, Yoshua and Louradour, J{\'e}r{\^o}me and Collobert, Ronan and Weston, Jason},
 title = {Curriculum Learning},
 url = {http://doi.acm.org/10.1145/1553374.1553380},
 pages = {41--48},
 publisher = {ACM},
 isbn = {978-1-60558-516-1},
 series = {ICML '09},
 booktitle = {Proceedings of the 26th Annual International Conference on Machine Learning},
 year = {2009},
 address = {New York, NY, USA},
 doi = {10.1145/1553374.1553380}
}


@article{Besold.2017,
 abstract = {This article aims to achieve two goals: to show that probability is not the only way of dealing with uncertainty (and even more, that there are kinds of uncertainty which are for principled reasons not addressable with probabilistic means); and to provide evidence that logic-based methods can well support reasoning with uncertainty. For the latter claim, two paradigmatic examples are presented: logic programming with Kleene semantics for modelling reasoning from information in a discourse, to an interpretation of the state of affairs of the intended model, and a neural-symbolic implementation of input/output logic for dealing with uncertainty in dynamic normative contexts.},
 author = {Besold, Tarek R. and Garcez, Artur S. Avila and Stenning, Keith and {van der Torre}, Leendert and {van Lambalgen}, Michiel},
 year = {2017},
 title = {Reasoning in Non-probabilistic Uncertainty: Logic Programming and Neural-Symbolic Computing as Examples},
 pages = {37--77},
 volume = {27},
 number = {1},
 issn = {1572-8641},
 journal = {Minds and Machines},
 doi = {10.1007/s11023-017-9428-3}
}


@book{Bessiere.2016,
 year = {2016},
 title = {Data Mining and Constraint Programming: Foundations of a Cross-Disciplinary Approach},
 publisher = {{Springer International Publishing}},
 isbn = {978-3-319-50137-6},
 series = {Lecture Notes in Artificial Intelligence},
 editor = {Bessiere, Christian and de Raedt, Luc and Kotthoff, Lars and Nijssen, Siegfried and O'Sullivan, Barry and Pedreschi, Dino},
 doi = {10.1007/978-3-319-50137-6}
}


@book{C.Cortes.2015,
 year = {2015},
 title = {Advances in Neural Information Processing Systems 28},
 publisher = {{Curran Associates, Inc}},
 editor = {{C. Cortes} and {N. D. Lawrence} and {D. D. Lee} and {M. Sugiyama} and {R. Garnett}}
}


@article{Cano.2017,
 abstract = {Feature extraction transforms high-dimensional data into a new subspace of lower dimensionality while keeping the classification accuracy. Traditional algorithms do not consider the multi-objective nature of this task. Data transformations should improve the classification performance on the new subspace, as well as to facilitate data visualization, which has attracted increasing attention in recent years. Moreover, new challenges arising in data mining, such as the need to deal with imbalanced data sets call for new algorithms capable of handling this type of data. This paper presents a Pareto-based multi-objective genetic programming algorithm for feature extraction and data visualization. The algorithm is designed to obtain data transformations that optimize the classification and visualization performance both on balanced and imbalanced data. Six classification and visualization measures are identified as objectives to be optimized by the multi-objective algorithm. The algorithm is evaluated and compared to 11 well-known feature extraction methods, and to the performance on the original high-dimensional data. Experimental results on 22 balanced and 20 imbalanced data sets show that it performs very well on both types of data, which is its significant advantage over existing feature extraction algorithms.},
 author = {Cano, Alberto and Ventura, Sebasti{\'a}n and Cios, Krzysztof J.},
 year = {2017},
 title = {Multi-objective genetic programming for feature extraction and data visualization},
 pages = {2069--2089},
 volume = {21},
 number = {8},
 issn = {1433-7479},
 journal = {Soft Computing},
 doi = {10.1007/s00500-015-1907-y}
}


@proceedings{CEURWS.org.2015,
 year = {2015},
 title = {Proceedings of the 2015th International Conference on Cognitive Computation: Integrating Neural and Symbolic Approaches-Volume 1583},
 institution = {{CEUR-WS. org}}
}


@inproceedings{Chakraborty.2017,
 author = {Chakraborty, Supriyo and Tomsett, Richard and Raghavendra, Ramya and Harborne, Daniel and Alzantot, Moustafa and Cerutti, Federico and Srivastava, Mani and Preece, Alun and Julier, Simon and Rao, Raghuveer M. and others},
 title = {Interpretability of deep learning models: a survey of results},
 booktitle = {IEEE Smart World Congress 2017 Workshop: DAIS},
 year = {2017}
}


@inproceedings{Clos.,
 author = {Clos, J{\'e}r{\'e}mie and Wiratunga, Nirmalie and Massie, Stewart},
 title = {Towards Explainable Text Classification by Jointly Learning Lexicon and Modifier Terms},
 pages = {19},
 booktitle = {IJCAI-17 Workshop on Explainable AI (XAI)}
}


@inproceedings{Datta.2016,
 author = {Datta, Anupam and Sen, Shayak and Zick, Yair},
 title = {Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems},
 keywords = {indirect influence},
 pages = {598--617},
 booktitle = {Security and Privacy (SP), 2016 IEEE Symposium on},
 year = {2016}
}


@article{Davis.2015,
 author = {Davis, Ernest and Marcus, Gary},
 year = {2015},
 title = {Commonsense Reasoning and Commonsense Knowledge in Artificial Intelligence},
 url = {http://doi.acm.org/10.1145/2701413},
 keywords = {general intelligence},
 pages = {92--103},
 volume = {58},
 number = {9},
 issn = {0001-0782},
 journal = {Commun. ACM},
 doi = {10.1145/2701413}
}


@article{Fong.2017,
 author = {Fong, Ruth C. and Vedaldi, Andrea},
 year = {2017},
 title = {Interpretable explanations of black boxes by meaningful perturbation},
 keywords = {pertubation},
 journal = {arXiv preprint arXiv:1704.03296}
}


@inproceedings{Zhang.2015,
 author = {Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
 title = {Character-level convolutional networks for text classification},
 pages = {649--657},
 booktitle = {Advances in neural information processing systems},
 year = {2015}
}


